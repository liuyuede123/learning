# mysql索引

## 磁盘io预读

考虑到磁盘IO是非常高昂的操作，计算机操作系统做了一些优化，**当一次IO时，不光把当前磁盘地址的数据，而是把相邻的数据也都读取到内存缓冲区内**，因为局部预读性原理告诉我们，当计算机访问一个地址的数据的时候，与其相邻的数据也会很快被访问到。每一次IO读取的数据我们称之为一页(page)。具体一页有多大数据跟操作系统有关，一般为4k或8k，也就是我们读取一页内的数据时候，实际上才发生了一次IO，这个理论对于索引的数据结构设计非常有帮助。

## 索引的数据结构

**B-数**

1. 关键字和数据都在节点中
2. 搜索可能在非叶子节点结束
3. 搜索性能相当于二分查找

**B+树**

1. 非叶子节点的子树指针 P[i]，指向关键字属于 **[k[i],K[i+1])** 的子树（**注意：区间是前闭后开**)。
2. 所有的关键字 **都出现在叶子节点的链表中**，且链表中的关键字是有序的。
3. **搜索只在叶子节点命中**
4. 非叶子节点相当于是 **叶子节点的索引层**，叶子节点是 **存储关键字数据的数据层**。
5. **B+树只需要去遍历叶子节点就可以实现整棵树的遍历**。

## myisam存储引擎

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210819203146175.png" alt="image-20210819203146175" style="zoom:50%;" />



**主键索引**

1. 树中的叶子节点保存的是对应行的物理位置。通过该值，**==存储引擎能顺利地进行回表查询，得到一行完整记录==**。
2. 每个叶子也保存了指向下一个叶子的指针，从而 **方便叶子节点的范围遍历**。

**辅助索引**

在 MyISAM 中，主键索引和辅助索引在结构上没有任何区别，**==只是主键索引要求 key 是唯一的，而辅助索引的 key 可以重复==**。



## Innodb 存储引擎

**主键索引**

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210819203545543.png" alt="image-20210819203545543" style="zoom:50%;" />

1. InnoDB 主键索引中既存储了主健值，又存储了行数据。

**辅助索引**

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210819203715602.png" alt="image-20210819203715602" style="zoom:50%;" />

1. 对于辅助索引，InnoDB 采用的方式是在叶子节点中保存主键值，通过这个主键值来回表查询到一条完整记录，因此 **按辅助索引检索其实进行了二次查询，效率是没有主键索引高的**。

**查询原理**

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210819204224271.png" alt="image-20210819204224271" style="zoom:50%;" />

1. 每个节点相当于一个磁盘快，每次查询会先查磁盘快1，发生一次磁盘io
2. 查找第二层发生一次磁盘io，查找到叶子节点发生一次磁盘io，可能几百万条数据只发生3次磁盘io
3. 所以层数直接决定磁盘io的次数，也就是查询的时间。所以数据项的内容越大，可能层数就越高
4. 这也是为什么b+树要求把真实的数据放到叶子节点而不是内层节点，一旦放到内层节点，磁盘块的数据项会大幅度下降，导致树增高。当数据项等于1时将会退化成线性表。



# mysql（undo log、redo log、bin log）

## binlog

`binlog `用于记录数据库执行的写入性操作(不包括查询)信息，以二进制的形式保存在磁盘中。 `binlog `是 `mysql`
的逻辑日志，并且由 `Server `层进行记录，使用任何存储引擎的 `mysql `数据库都会记录 `binlog `日志。

- **逻辑日志**： 可以简单理解为记录的就是sql语句 。
- **物理日志**： `mysql `数据最终是保存在数据页中的，物理日志记录的就是数据页变更 。

`binlog `是通过追加的方式进行写入的，可以通过 `max_binlog_size `参数设置每个 `binlog`
文件的大小，当文件大小达到给定值之后，会生成新的文件来保存日志。

**binlog使用场景**

1. 主从复制：在 `Master `端开启 `binlog `，然后将 `binlog `发送到各个 `Slave `端， `Slave `端重放 `binlog `从而达到主从数据一致。
2. **数据恢复** ：通过使用 `mysqlbinlog `工具来恢复数据。

**binlog刷盘时机**

对于 `InnoDB `存储引擎而言，只有在事务提交时才会记录 `biglog `，此时记录还在内存中，那么 `biglog`
是什么时候刷到磁盘中的呢？ `mysql `通过 `sync_binlog `参数控制 `biglog `的刷盘时机，取值范围是 `0-N`
：

- 0：不去强制要求，由系统自行判断何时写入磁盘；
- 1：每次 `commit `的时候都要将 `binlog `写入磁盘；
- N：每N个事务，才会将 `binlog `写入磁盘。

从上面可以看出， `sync_binlog `最安全的是设置是 `1 `，这也是 `MySQL 5.7.7`
之后版本的默认值。但是设置一个大一些的值可以提升数据库性能，因此实际情况下也可以将值适当调大，牺牲一定的一致性来获取更好的性能。

## redo log

**为什么需要redo log**

事务里面的四大特性，原子性，隔离性，持久性，一致性

**持久性**：只要事务提交成功，对数据的修改就会永久保存下来，不可能因为某些原因再回到原来的状态。

所以，每次提交事务的时候，将该事务涉及修改的数据页全部刷新到磁盘中。这会有2个缺点：

1. 因为 `Innodb `是以 `页 `为单位进行磁盘交互的，而一个事务很可能只修改一个数据页里面的几个字节，这个时候将完整的数据页刷到磁盘的话，太浪费资源了！
2. 一个事务可能涉及修改多个数据页，并且这些数据页在物理上并不连续，使用随机IO写入性能太差！

因此 `mysql `设计了 `redo log `， **具体来说就是只记录事务对数据页做了哪些修改**
，这样就能完美地解决性能问题了(相对而言文件更小并且是顺序IO)。

**redo log基本概念**

1. redo log包括2部分，一个是内存中的日志缓冲( `redo log buffer `)，另一个是磁盘上的日志文件( ` redo log file `)

2. mysql `每执行一条 `DML `语句，先将记录写入 `redo log buffer `
   ，后续某个时间点再一次性将多个操作记录写到 `redo log file 。这种 **先写日志，再写磁盘** 的技术就是 `MySQL`里经常说到的 `WAL(Write-Ahead Logging) `技术。

3. 在计算机操作系统中，用户空间( `user space `)下的缓冲区数据一般情况下是无法直接写入磁盘的，中间必须经过操作系统内核空间( `
   kernel space `)缓冲区( `OS Buffer `)。因此， `redo log buffer `写入 `redo log
   file `实际上是先写入 `OS Buffer `，然后再通过系统调用 `fsync() `将其刷到 redo log file

4. `mysql `支持三种将 `redo log buffer `写入 `redo log file `的时机，可以通过 `
   innodb_flush_log_at_trx_commit ` 参数配置

   1. 0-延迟写，事务提交时不会将 `redo log buffer `中日志写入到 `os buffer `，而是每秒写入 `os buffer `并调用 `fsync() `写入到 `redo log file `中。也就是说设置为0时是(大约)每秒刷新写入到磁盘中的，当系统崩溃，会丢失1秒钟的数据。

   2. 1-实时写，实时刷。事务每次提交都会将 `redo log buffer `中的日志写入 `os buffer `并调用 `fsync() `刷到 `redo log file `中。这种方式即使系统崩溃也不会丢失任何数据，但是因为每次提交都写入磁盘，IO的性能较差。

   3. 2-实时写，延迟刷。每次提交都仅写入到 `os buffer `，然后是每秒调用 `fsync() `将 `os buffer `中的日志写入到 `redo log file `。

      <img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820125547858.png" alt="image-20210820125547858" style="zoom:50%;" />



**Redo log记录形式**

`redo log `实际上记录数据页的变更，而这种变更记录是没必要全部保存，因此 `redo log`实现上采用了大小固定，循环写入的方式，当写到结尾时，会回到开头循环写日志。如下图：

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820130004161.png" alt="image-20210820130004161" style="zoom:50%;" />



1. 在innodb中，既有` redo log `需要刷盘，还有 `数据页 `也需要刷盘， `redo log `存在的意义主要就是降低对 `数据页 `刷盘的要求 **。**
2. **在上图中， `write pos `表示 `redo log `当前记录的 `LSN` (逻辑序列号)位置， `check point `表示** 数据页更改记录** 刷盘后对应 `redo log `所处的 `LSN `(逻辑序列号)位置
3. `write pos `到 `check point `之间的部分是 `redo log `空着的部分，用于记录新的记录；` check point `到 `write pos `之间是 `redo log `待落盘的数据页更改记录
4. 当 `write pos `追上 `check point `时，会先推动 `check point `向前移动，空出位置再记录新的日志。
5. 启动 `innodb `的时候，不管上次是正常关闭还是异常关闭，总是会进行恢复操作。因为 `redo log `记录的是数据页的物理变化，因此恢复的时候速度比逻辑日志(如 `binlog `)要快很多
6. 重启 `innodb `时，首先会检查磁盘中数据页的 `LSN `，如果数据页的 `LSN `小于日志中的 `LSN `，则会从 `checkpoint `开始恢复。
7. 在宕机前正处于
   `checkpoint `的刷盘过程，且数据页的刷盘进度超过了日志页的刷盘进度，此时会出现数据页中记录的 `LSN `大于日志中的 `LSN`
   ，这时超出日志进度的部分将不会重做，因为这本身就表示已经做过的事情，无需再重做。



**redo log与binlog区别**

| redo log | binlog                                                       |                                                              |
| -------- | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 文件大小 | `redo log `的大小是固定的。                                  | `binlog `可通过配置参数 `max_binlog_size `设置每个` binlog `文件的大小。 |
| 实现方式 | `redo log `是 `InnoDB `引擎层实现的，并不是所有引擎都有。    | `binlog `是 `Server` 层实现的，所有引擎都可以使用 `binlog `日志 |
| 记录方式 | redo log 采用循环写的方式记录，当写到结尾时，会回到开头循环写日志。 | binlog通过追加的方式记录，当文件大小大于给定值后，后续的日志会记录到新的文件上 |
| 适用场景 | `redo log `适用于崩溃恢复(crash-safe)                        | `binlog `适用于主从复制和数据恢复                            |



## undo log

**undo log 基本概念**

数据库事务四大特性中有一个是 **原子性** ，具体来说就是 **原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况**
。实际上， **原子性** 底层就是通过 `undo log `实现的。 `undo log `主要记录了数据的逻辑变化，比如一条 ` INSERT
`语句，对应一条 `DELETE `的 `undo log `，对于每个 `UPDATE `语句，对应一条相反的 `UPDATE `的`
undo log `，这样在发生错误时，就能回滚到事务之前的数据状态。同时， `undo log `也是 `MVCC `
(多版本并发控制)实现的关键

当执行 rollback 时，就可以从undo log中的逻辑记录读取到相应的内容并进行回滚。有时候应用到行版本控制的时候，也是通过undo log来实现的：当读取的某一行被其他事务锁定时，它可以从undo log中分析出该行记录以前的数据是什么，从而提供该行版本信息，让用户实现非锁定一致性读取。

**undo log是采用段(segment)的方式来记录的，每个undo操作在记录的时候占用一个undo log segment**

另外，**undo log**也会产生redo log**，因为undo log**也要实现持久性保护。

**delete/update操作的内部机制**

1. 当事务提交的时候，innodb不会立即删除undo log，因为后续还可能会用到undo log，如隔离级别为repeatable read时，事务读取的都是开启事务时的最新提交行版本，只要该事务不结束，该行版本就不能删除，即undo log不能删除。
2. 但是在事务提交的时候，会将该事务对应的undo log放入到删除列表中，未来通过purge来删除。并且提交事务时，还会判断undo log分配的页是否可以重用，如果可以重用，则会分配给后面来的事务，避免为每个独立的事务分配独立的undo log页而浪费存储空间和性能。
3. 通过undo log记录delete和update操作的结果发现：(insert操作无需分析，就是插入行而已)
   - delete操作实际上不会直接删除，而是将delete对象打上delete flag，标记为删除，最终的删除操作是purge线程完成的。
   - update分为两种情况：update的列是否是主键列。
   - 如果不是主键列，在undo log中直接反向记录是如何update的。即update是直接进行的。
   - 如果是主键列，update分两部执行：先删除该行，再插入一行目标行。

**binlog和事务日志的先后顺序及group commit**

1. 提交事务时，在存储引擎层的上一层结构中会将事务按序放入一个队列，队列中的第一个事务称为 leader，其他事务称为 follower，leader 控制着 follower 的行为。虽然顺序还是一样先刷二进制，再刷事务日志，但是机制完全改变了：删除了原来的prepare_commit_mutex 行为，也能保证即使开启了二进制日志，group commit 也是有效的。



## 两阶段提交

## 为什么需要两阶段提交？

假设当前 ID=2 的行，字段 c 的值是 0，再假设执行 update 语句过程中在写完第一个日志后，第二个日志还没有写完期间发生了 crash，会出现什么情况呢？

1. 先写 redo log 后写 binlog。假设在 redo log 写完，binlog 还没有写完的时候，MySQL 进程异常重启。由于我们前面说过的，redo log 写完之后，系统即使崩溃，仍然能够把数据恢复回来，所以恢复后这一行 c 的值是 1。但是由于 binlog 没写完就 crash 了，这时候 binlog 里面就没有记录这个语句。因此，之后备份日志的时候，存起来的 binlog 里面就没有这条语句。然后你会发现，如果需要用这个 binlog 来恢复临时库的话，由于这个语句的 binlog 丢失，这个临时库就会少了这一次更新，恢复出来的这一行 c 的值就是 0，与原库的值不同。
2. 先写 binlog 后写 redo log。如果在 binlog 写完之后 crash，由于 redo log 还没写，崩溃恢复以后这个事务无效，所以这一行 c 的值是 0。但是 binlog 里面已经记录了“把 c 从 0 改成 1”这个日志。所以，在之后用 binlog 来恢复的时候就多了一个事务出来，恢复出来的这一行 c 的值就是 1，与原库的值不同。

可以看到，如果不使用“两阶段提交”，那么数据库的状态就有可能和用它的日志恢复出来的库的状态不一致。



<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820153539909.png" alt="image-20210820153539909" style="zoom:50%;" />

崩溃恢复

如果在图中时刻 A 的地方，也就是写入 redo log 处于 prepare 阶段之后、写 binlog 之前，发生了崩溃（crash），由于此时 binlog 还没写，redo log 也还没提交，所以崩溃恢复的时候，这个事务会回滚。这时候，binlog 还没写，所以也不会传到备库。

如果 redo log 里面的事务是完整的，也就是已经有了 commit 标识，则直接提交；如果 redo log 里面的事务只有完整的 prepare，则判断对应的事务 binlog 是否存在并完整：

- a. 如果是，则提交事务；
- b. 否则，回滚事务。

这里，时刻 B 发生 crash 对应的就是 2(a) 的情况，崩溃恢复过程中事务会被提交。

ps: 两阶段提交的最后一个阶段的操作本身是不会失败的，除非是系统或硬件错误，所以也就不再需要回滚（不然就可以无限循环下去了）



# mysql事务

## 隐式事务

隐式事务(mysql会自动为增删改语句加事务)

## 事务四大特性

事务包含四大特性，即**原子性（Atomicity）**、**一致性（Consistency）**、**隔离性（Isolation）\**和\**持久性（Durability）**(ACID)。

1. 原子性（Atomicity） **原子性是指对数据库的一系列操作，要么全部成功，要么全部失败，不可能出现部分成功的情况**。以转账场景为例，一个账户的余额减少，另一个账户的余额增加，这两个操作一定是同时成功或者同时失败的。**undo log实现**
2. 一致性（Consistency） **一致性是指数据库的完整性约束没有被破坏，在事务执行前后都是合法的数据状态**。这里的一致可以表示数据库自身的约束没有被破坏，比如某些字段的唯一性约束、字段长度约束等等；还可以表示各种实际场景下的业务约束，比如上面转账操作，一个账户减少的金额和另一个账户增加的金额一定是一样的。（**另外3中特性保证**）
3. 隔离性（Isolation） **隔离性指的是多个事务彼此之间是完全隔离、互不干扰的**。隔离性的最终目的也是为了保证一致性。**read view实现**
4. 持久性（Durability） **持久性是指只要事务提交成功，那么对数据库做的修改就被永久保存下来了，不可能因为任何原因再回到原来的状态**。**redo log实现

## 事务的状态

1. 活动的（active）：当事务对应的数据库操作正在执行过程中。
2. 部分提交的（partially committed）：当事务中的最后一个操作完成，但是还未将变更刷新到磁盘的时候。
3. 失败的（failed）：当事务处于活动或者部分提交状态时，由于某些错误导致事务无法执行。
4. 中止的（aborted）：当事务处于失败状态，且回滚操作执行完毕，数据恢复到事务执行之前的状态。
5. 提交的（committed）：当事务处于部分提交状态，并且将修改过的数据同步到磁盘之后。

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820141843088.png" alt="image-20210820141843088" style="zoom:50%;" />

## 事务隔离级别

### 事务并发执行遇到的问题

1. 脏写（Dirty Write） **脏写是指一个事务修改了其它事务未提交的数据**。
2. 脏读（Dirty Read） **脏读是指一个事务读到了其它事务未提交的数据**。
3. 不可重复读（Non-Repeatable Read） **不可重复读指的是在一个事务执行过程中，读取到其它事务已提交的数据，导致两次读取的结果不一致**。
4. 幻读（Phantom） **幻读是指的是在一个事务执行过程中，读取到了其他事务新插入数据，导致两次读取的结果不一致**。
5. 不可重复读和幻读的区别在于**不可重复读是读到的是其他事务修改或者删除的数据，而幻读读到的是其它事务新插入的数据**。

### 四种隔离级别

- **`READ UNCOMMITTED`：未提交读。**

带来的问题：脏读，不可重复读，幻读

- **`READ COMMITTED`：已提交读。**

带来的问题：不可重复读，幻读

- **`REPEATABLE READ`：可重复读。**

带来的问题：幻读（对innodb不可能）

- `SERIALIZABLE`：串行化。

### MVCC（多版本并发控制）

**版本链**

在`InnoDB`中，每行记录实际上都包含了两个隐藏字段：事务id(`trx_id`)和回滚指针(`roll_pointer`)。

1. `trx_id`：事务id。每次修改某行记录时，都会把该事务的事务id赋值给`trx_id`隐藏列。
2. `roll_pointer`：回滚指针。每次修改某行记录时，都会把`undo`日志地址赋值给`roll_pointer`隐藏列。

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820143138952.png" alt="image-20210820143138952" style="zoom:50%;" />

假设之后两个事务`id`分别为`100`、`200`的事务对这条记录进行`UPDATE`操作，操作流程如下：

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820143632035.png" alt="image-20210820143632035" style="zoom:50%;" />

由于每次变动都会先把`undo`日志记录下来，并用`roll_pointer`指向`undo`日志地址。因此可以认为，**对该条记录的修改日志串联起来就形成了一个`版本链`，版本链的头节点就是当前记录最新的值**。如下：

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820143659524.png" alt="image-20210820143659524" style="zoom:50%;" />



**ReadView**

如果数据库隔离级别是`未提交读（READ UNCOMMITTED）`，那么读取版本链中最新版本的记录即可。如果是是`串行化（SERIALIZABLE）`，事务之间是加锁执行的，不存在读不一致的问题。**但是如果是`已提交读（READ COMMITTED）`或者`可重复读（REPEATABLE READ）`，就需要遍历版本链中的每一条记录，判断该条记录是否对当前事务可见，直到找到为止(遍历完还没找到就说明记录不存在)**。`InnoDB`通过`ReadView`实现了这个功能。`ReadView`中主要包含以下4个内容：

- `m_ids`：表示在生成`ReadView`时当前系统中活跃的读写事务的事务id列表。
- `min_trx_id`：表示在生成`ReadView`时当前系统中活跃的读写事务中最小的事务id，也就是`m_ids`中的最小值。
- `max_trx_id`：表示生成`ReadView`时系统中应该分配给**下一个事务的id值**。
- `creator_trx_id`：表示生成该`ReadView`事务的事务id。

有了`ReadView`之后，我们可以基于以下步骤判断某个版本的记录是否对当前事务可见。

1. 如果被访问版本的`trx_id`属性值与`ReadView`中的`creator_trx_id`值相同，意味着当前事务在访问它自己修改过的记录，所以该版本可以被当前事务访问。
2. 如果被访问版本的`trx_id`属性值小于`ReadView`中的`min_trx_id`值，表明生成该版本的事务在当前事务生成`ReadView`前已经提交，所以该版本可以被当前事务访问。
3. 如果被访问版本的`trx_id`属性值大于或等于`ReadView`中的`max_trx_id`值，表明生成该版本的事务在当前事务生成`ReadView`后才开启，所以该版本不可以被当前事务访问。
4. 如果被访问版本的`trx_id`属性值在`ReadView`的`min_trx_id`和`max_trx_id`之间，那就需要判断一下`trx_id`属性值是不是在`m_ids`列表中，如果在，说明创建`ReadView`时生成该版本的事务还是活跃的，该版本不可以被访问；如果不在，说明创建`ReadView`时生成该版本的事务已经被提交，该版本可以被访问。

**注意**

1. 在`MySQL`中，`READ COMMITTED`和`REPEATABLE READ`隔离级别的的一个非常大的区别就是它们生成`ReadView`的时机不同。
2. **`READ COMMITTED`在每次读取数据前都会生成一个`ReadView`**，这样就能保证每次都能读到其它事务已提交的数据。
3. **`REPEATABLE READ` 只在第一次读取数据时生成一个`ReadView`**，这样就能保证后续读取的结果完全一致。因为后面再有更新数据的提交，事务id会比rearview的高，所以不能被当前事务访问。

# mysql锁

事务并发访问同一数据资源的情况主要就分为`读-读`、`写-写`和`读-写`三种。

1. `读-读` 即并发事务同时访问同一行数据记录。由于两个事务都进行只读操作，不会对记录造成任何影响，因此并发读完全允许。
2. `写-写` 即并发事务同时修改同一行数据记录。这种情况下可能导致`脏写`问题，这是任何情况下都不允许发生的，因此只能通过`加锁`实现，也就是当一个事务需要对某行记录进行修改时，首先会先给这条记录加锁，如果加锁成功则继续执行，否则就排队等待，事务执行完成或回滚会自动释放锁。
3. `读-写` 即一个事务进行读取操作，另一个进行写入操作。这种情况下可能会产生`脏读`、`不可重复读`、`幻读`。最好的方案是**读操作利用多版本并发控制（`MVCC`），写操作进行加锁**。

**锁的粒度**

按锁作用的数据范围进行分类的话，锁可以分为`行级锁`和`表级锁`。

1. `行级锁`：作用在数据行上，锁的粒度比较小。
2. `表级锁`：作用在整张数据表上，锁的粒度比较大。

**锁的分类**

为了实现`读-读`之间不受影响，并且`写-写`、`读-写`之间能够相互阻塞，`Mysql`使用了`读写锁`的思路进行实现，具体来说就是分为了`共享锁`和`排它锁`：

还需要注意的一点是，如果一个事务已经持有了某行记录的`S锁`，另一个事务是无法为这行记录加上`X锁`的，反之亦然。

**`InnoDB`的行锁，是通过锁住索引来实现的，如果加锁查询的时候没有使用过索引，会将整个聚簇索引都锁住，相当于锁表了**。

1. `共享锁(Shared Locks)`：简称`S锁`，在事务要读取一条记录时，需要先获取该记录的`S锁`。`S锁`可以在同一时刻被多个事务同时持有。我们可以用`select ...... lock in share mode;`的方式手工加上一把`S锁`。
2. `排他锁(Exclusive Locks)`：简称`X锁`，在事务要改动一条记录时，需要先获取该记录的`X锁`。`X锁`在同一时刻最多只能被一个事务持有。`X锁`的加锁方式有两种，第一种是自动加锁，在对数据进行增删改的时候，都会默认加上一个`X锁`。还有一种是手工加锁，我们用一个`FOR UPDATE`给一行数据加上一个`X锁`。
3. **`意向锁`可以认为是`S锁`和`X锁`在数据表上的标识，通过意向锁可以快速判断表中是否有记录被上锁，从而避免通过遍历的方式来查看表中有没有记录被上锁，提升加锁效率**。例如，我们要加表级别的`X锁`，这时候数据表里面如果存在行级别的`X锁`或者`S锁`的，加锁就会失败，此时直接根据`意向锁`就能知道这张表是否有行级别的`X锁`或者`S锁`。
4. 记录锁：就是指聚簇索引中真实存放的数据，比如上面的1、4、7、10都是记录。
5. 间隙锁(Gap Locks) 间隙指的是两个记录之间逻辑上尚未填入数据的部分。比如上述的(1,4)、(4,7)等。
6. 临键锁(Next-Key Locks) 临键指的是间隙加上它右边的记录组成的左开右闭区间。比如上述的(1,4]、(4,7]等。

`间隙锁(Gap Locks)`和`临键锁(Next-Key Locks)`都是用来解决幻读问题的，在`已提交读（READ COMMITTED）`隔离级别下，`间隙锁(Gap Locks)`和`临键锁(Next-Key Locks)`都会失效！

# mysql主从同步

## 复制过程

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820165400695.png" alt="image-20210820165400695" style="zoom:50%;" />



1. 在从节点上执行 `sart slave` 命令开启主从复制开关，开始进行主从复制。从节点上的 I/O 进程连接主节点，并请求从指定日志文件的指定位置（或者从最开始的日志）之后的日志内容。
2. 主节点接收到来自从节点的 I/O 请求后，通过负责复制的 I/O 进程（log Dump Thread）根据请求信息读取指定日志指定位置之后的日志信息，返回给从节点。返回信息中除了日志所包含的信息之外，还包括本次返回的信息的 Binlog file 以及 Binlog position（Binlog 下一个数据读取位置）。
3. 从节点的 I/O 进程接收到主节点发送过来的日志内容、日志文件及位置点后，将接收到的日志内容更新到本机的 relay log 文件（Mysql-relay-bin.xxx）的最末端，并将读取到的 Binlog文件名和位置保存到`master-info` 文件中，以便在下一次读取的时候能够清楚的告诉 Master ：“ 我需要从哪个 Binlog 的哪个位置开始往后的日志内容，请发给我”。
4. Slave 的 SQL 线程检测到relay log 中新增加了内容后，会将 relay log 的内容解析成在能够执行 SQL 语句，然后在本数据库中按照解析出来的顺序执行，并在 `relay log.info` 中记录当前应用中继日志的文件名和位置点。

## 主从复制的模式

### 异步模式 (async-mode)

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820165924024.png" alt="image-20210820165924024" style="zoom:50%;" />



1. 主节点不主动推送数据到从节点，主库在执行完客户端提交的事务后会立即将结果返给给客户端，并不关心从库是否已经接收并处理，这样就会有一个问题，主节点如果崩溃掉了，此时主节点上已经提交的事务可能并没有传到从节点上，如果此时，强行将从提升为主，可能导致新主节点上的数据不完整。

### 半同步模式(semi-sync)

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820170158467.png" alt="image-20210820170158467" style="zoom:50%;" />

1. 介于异步复制和全同步复制之间，主库在执行完客户端提交的事务后不是立刻返回给客户端，而是等待至少一个从库接收到并写到 relay log 中才返回成功信息给客户端（只能保证主库的 Binlog 至少传输到了一个从节点上），否则需要等待直到超时时间然后切换成异步模式再提交。
2. 相对于异步复制，半同步复制提高了数据的安全性，一定程度的保证了数据能成功备份到从库，同时它也造成了一定程度的延迟，但是比全同步模式延迟要低，这个延迟最少是一个 TCP/IP 往返的时间。所以，半同步复制最好在低延时的网络中使用。
3. 半同步模式不是 MySQL 内置的，从 MySQL 5.5 开始集成，需要 master 和 slave 安装插件开启半同步模式

### 全同步模式

指当主库执行完一个事务，然后所有的从库都复制了该事务并成功执行完才返回成功信息给客户端。因为需要等待所有从库执行完该事务才能返回成功信息，所以全同步复制的性能必然会收到严重的影响。

### 并行复制

并行复制的本质是同时执行的SQL不存在锁争用。

<img src="https://raw.githubusercontent.com/liuyuede123/image/main/image-20210820190509277.png" alt="image-20210820190509277" style="zoom:50%;" />

1. 在MySQL中，复制线程是由参数slave_parallel_workers来控制的，通常情况下，在8G内存、8核CPU的机器上，将该值设置为8比较合适，如果你的CPU核数比较高，那么可以适当调整为8~16之间的数字。
2. 主库上能够并行提交的事务，也就是已经进入到了redo log commit阶段的事务，在从库上也一定能够并行提交，所以在主库上并行提交的事务，它用一个commit_id对这组事务来进行标识，下一组并行事务的commit_id为本组的commit_id+1
3. last_committed表示事务提交的时候，上次事务提交的编号，如果事务具有相同的last_committed，表示这些事务都在一组内，可以进行并行的回放。例如上述last_committed为0的事务有6个，表示组提交时提交了6个事务，而这6个事务在从机是可以进行并行回放的。
4. MySQL5.7的并行复制时将所有在主库上处于redo log prepare阶段的事务，和该阶段之后的事务，也就是处于redo log commit阶段的事务，在从库并行执行，从而减少worker线程不必要的等待。

### GTID复制模式

**GTID (global transaction identifier)** 即全局事务 ID，一个事务对应一个 GTID，保证了在每个在主库上提交的事务在集群中有一个唯一的 ID。

1. 在传统的复制里面，当发生故障，需要主从切换，需要找到binlog和pos点，然后将主节点指向新的主节点，相对来说比较麻烦，也容易出错。在MySQL 5.6里面，不用再找binlog和pos点，我们只需要知道主节点的ip，端口，以及账号密码就行，因为复制是自动的，MySQL会通过内部机制GTID自动找点同步。
2. 主节点更新数据时，会在事务前产生GTID，一起记录到binlog日志中。
3. 从节点的I/O线程将变更的bin log，写入到本地的relay log中。
4. SQL线程从relay log中获取GTID，然后对比本地binlog是否有记录（所以MySQL从节点必须要开启binary log）。
5. 如果有记录，说明该GTID的事务已经执行，从节点会忽略。
6. 如果没有记录，从节点就会从relay log中执行该GTID的事务，并记录到bin log。
7. 在解析过程中会判断是否有主键，如果没有就用二级索引，如果有就用全部扫描。



# mysql集群